由BERT到Bi-LSTM的知识蒸馏
============


整体原理介绍
------------

本例是将特定任务下BERT模型的知识蒸馏到基于Bi-LSTM的小模型中，主要参考论文 `Distilling Task-Specific Knowledge from BERT into Simple Neural Networks`实现。整体原理如下：
    
1. 在本例中，较大的模型是BERT被称为教师模型，Bi-LSTM被称为学生模型。

2. 小模型学习大模型的知识，需要小模型学习蒸馏相关的损失函数。在本实验中，损失函数是均方误差损失函数，传入函数的两个参数分别是学生模型的输出和教师模型的输出。

3. 在论文的模型蒸馏阶段，作者为了能让教师模型表达出更多的“暗知识”(dark knowledge，通常指分类任务中低概率类别与高概率类别的关系)供学生模型学习，对训练数据进行了数据增强。通过数据增强，可以产生更多无标签的训练数据，在训练过程中，学生模型可借助教师模型的“暗知识”，在更大的数据集上进行训练，产生更好的蒸馏效果。本文的作者使用了三种数据增强方式，分别是：

   - Masking，即以一定的概率将原数据中的word token替换成 ``[MASK]`` ；

   - POS—guided word replacement，即以一定的概率将原数据中的词用与其有相同POS tag的词替换；

   - n-gram sampling，即以一定的概率，从每条数据中采样n-gram，其中n的范围可通过人工设置。

